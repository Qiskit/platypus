{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instances and extensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter will cover some of the different quantum variational algorithms:\n",
    "\n",
    "* [Variational Quantum Eigensolver (VQE)](https://arxiv.org/abs/1304.3061)\n",
    "* [Subspace Search VQE (SSVQE)](https://arxiv.org/abs/1810.09434)\n",
    "* [Variational Quantum Deflation (VQD)](https://arxiv.org/abs/1805.08138)\n",
    "* [Quantum Sampling Regression (QSR)](https://arxiv.org/pdf/2012.02338)\n",
    "\n",
    "Using these algorithms, we'll learn about a few design ideas to include in a custom variational algorithm: weights, overlap, and oversampling / undersampling. We encourage you to experiment with these ideas and share your findings with the community!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Quantum Eigensolver (VQE)\n",
    "\n",
    "[VQE](https://arxiv.org/abs/1304.3061) is one of the most widely used variational quantum algorithms, setting up a template for other algorithms to build upon. \n",
    "\n",
    "![VQE](images/instances_VQE.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VQE's layout is simple:\n",
    "\n",
    "- Prepare reference with $U_R$\n",
    "  - We'll go from the state $|0\\rangle$ to the reference state $|\\rho\\rangle$\n",
    "- Apply variational form $U_V(\\vec\\theta_{i,j})$ to create an ansatz $U_A(\\vec\\theta_{i,j})$\n",
    "  - We'll go from the state $|\\rho\\rangle$ to $U_V(\\vec\\theta_{i,j})|\\rho\\rangle = |\\psi(\\vec\\theta_{i,j})\\rangle$\n",
    "- Bootstrap at $i=0$ if we have a similar problem (typically found via classical simulation or sampling)\n",
    "  - Each optimizer will be bootstrapped in a different way, leading to an initial set of parameter vectors $\\Theta_0 := \\{\\vec\\theta_{0,j} | j \\in \\mathcal{J}_\\text{opt}^0\\}$ (e.g. from an initial point $\\vec\\theta_0$).\n",
    "- Evaluate the cost function $C(\\vec\\theta_{i,j}) := \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ for all prepared states on a quantum computer.\n",
    "- Use a classical optimizer to choose the next set of parameters $\\Theta_{i+1}$.\n",
    "- Repeat until convergence is reached.\n",
    "\n",
    "This is a simple classical optimization loop where we evaluate the cost function. Some optimizers could require several evaluations to calculate a gradient, determine the next iteration, or assess convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subspace Search VQE (SSVQE)\n",
    "\n",
    "[SSVQE](https://arxiv.org/abs/1810.09434) is a variant of VQE that enables you to obtain the first $k$ eigenvalues of an observable $\\hat{H}$ with eigenvalues $\\{\\lambda_0, \\lambda_1,...,\\lambda_{N-1}\\}$, with $N\\geq k$. We'll assume without loss of generality that $\\lambda_0<\\lambda_1<...<\\lambda_{N-1}$.\n",
    "\n",
    "SSQVE introduces a new idea: adding weights to help prioritise optimizing for the term with the largest weight.\n",
    "\n",
    "![SSVQE](images/instances_SSVQE.png)\n",
    "\n",
    "This algorithm requires the usage of $k$ mutually orthogonal reference states $|\\rho_j\\rangle_{j=0}^{k-1}$, that is, $\\langle \\rho_j | \\rho_l \\rangle = \\delta_{jl}$ for $j,l<k$. We can construct this easily by using our Pauli operators. The cost function of this algorithm is then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C(\\vec{\\theta}) \n",
    "\n",
    "& := \\sum_{j=0}^{k-1} w_j \\langle \\rho_j | U_{V}^{\\dagger}(\\vec{\\theta})\\hat{H} U_{V}(\\vec{\\theta})|\\rho_j \\rangle \\\\[1mm]\n",
    "\n",
    "& := \\sum_{j=0}^{k-1} w_j \\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle \\\\[1mm]\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $w_j$ is an arbitrary positive number such that if $j<l<k$ then $w_j>w_l$ and $U_{V}(\\vec{\\theta})$ is the user-defined variational form.\n",
    "\n",
    "SSVQE hinges on the idea that eigenstates that correspond to different eigenvalues are mutually orthogonal. In particular, the inner product of $U_{V}(\\vec{\\theta})|\\rho_j\\rangle$ and $U_{V}(\\vec{\\theta})|\\rho_l\\rangle$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle \\rho_j | U_{V}^{\\dagger}(\\vec{\\theta})U_{V}(\\vec{\\theta})|\\rho_l \\rangle\n",
    "\n",
    "& = \\langle \\rho_j | I |\\rho_l \\rangle \\\\[1mm]\n",
    "\n",
    "& = \\langle \\rho_j \\rho_l \\rangle \\\\[1mm]\n",
    "\n",
    "& = \\delta_{jl}\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first equality holds because $U_{V}(\\vec{\\theta})$ is a quantum operator and is therefore unitary. The last equality hold because of the orthogonality of the reference states $|\\rho_j\\rangle$. The fact that orthogonality is preserved through unitary transformations is deeply related to the principle of conservation of information, as expressed in quantum information science. Under this view, non-unitary transformations represent processes where information is either lost or injected.\n",
    "\n",
    "Weights $w_j$ help ensure that those states are all eigenstates. If the weights are different enough, the term with largest weight (i.e. $w_0$) will be given priority during optimization over the rest, so the resulting state $U_{V}(\\vec{\\theta})|\\rho_0 \\rangle$ will end up becoming the eigenstate corresponding to $\\lambda_0$. Due to the mutual orthogonality of $U_{V}(\\vec{\\theta})|\\rho_j\\rangle_{j=0}^{k-1}$, the rest of the states will be orthogonal to it and, therefore, contained in the subspace corresponding to the eigenvalues $\\{\\lambda_1,...,\\lambda_{N-1}\\}$.\n",
    "\n",
    "Applying the same argument to the rest of the terms, the next priority would then be the term with the weight $w_1$, so $U_{V}(\\vec{\\theta})|\\rho_1 \\rangle$ would be the eigenstate corresponding to $\\lambda_1$ and the other terms would be contained in the eigenspace of $\\{\\lambda_2,...,\\lambda_{N-1}\\}$.\n",
    "\n",
    "Reasoning inductively, we deduce that $U_{V}(\\vec{\\theta})|\\rho_j \\rangle$ will be an approximate eigenstate of $\\lambda_j$ for $0\\leq j < k $.\n",
    "\n",
    "SSVQE's layout is as follows:\n",
    "\n",
    "- Prepare several reference states with $U_R$\n",
    "   - This algorithm requires the usage of $k$ mutually orthogonal reference states $|\\rho_j\\rangle_{j=0}^{k-1}$, that is, $\\langle \\rho_j | \\rho_l \\rangle = \\delta_{jl}$ for $j,l<k$.\n",
    "- Apply variational form $U_V(\\vec\\theta_{i,j})$ to each reference state, creating the following ansatze $U_A(\\vec\\theta_{i,j})$\n",
    "- Bootstrap at $i=0$ if we have a similar problem (typically found via classical simulation or sampling)\n",
    "- Evaluate the cost function $C(\\vec\\theta_{i,j}) := \\sum_{j=0}^{k-1} w_j \\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ for all prepared states on a quantum computer\n",
    "  - This can be reasonably separated to calculating the expectation value for an observable $\\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$, and multiplying that result by $w_j$. \n",
    "  - After doing so, the cost function should return the sum of all weighted expecation values \n",
    "- Use a classical optimizer to choose the next set of parameters $\\Theta_{i+1}$.\n",
    "- Repeat until convergence is reached."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Quantum Deflation (VQD)\n",
    "\n",
    "[VQD](https://arxiv.org/abs/1805.08138) is an iterative method that extends VQE to obtain the $k$ first eigenvalues of an observable $\\hat{H}$ with eigenvalues $\\{\\lambda_0, \\lambda_1,...,\\lambda_{N-1}\\}$, with $N\\geq k$, instead of only the first. For the rest of this section we will assume without loss of generality that $\\lambda_0\\leq\\lambda_1\\leq...\\leq\\lambda_{N-1}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general idea behind VQD is that, first, you use VQE as usual to get the lowest eigenvalue $\\lambda_0 := C_0(\\vec\\theta^0) \\equiv C_\\text{VQE}(\\vec\\theta^0)$ with the corresponding (approximate) eigenstate $|\\psi(\\vec{\\theta^0})\\rangle$ for some optimal parameter vector $\\vec{\\theta^0}$. Then, in order to obtain the next eigenvalue $\\lambda_1 > \\lambda_0$, instead of minimizing the cost function $C_0(\\vec{\\theta}) := \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$, we optimize\n",
    "\n",
    "$$\n",
    "C_1(\\vec{\\theta}) := \n",
    "C_0(\\vec{\\theta})+ \\beta_0 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^0})\\rangle  |^2 \n",
    "$$\n",
    "\n",
    "<!-- $$\n",
    "= \n",
    "\\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle + \n",
    "\\beta_0 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^0})\\rangle  |^2\n",
    "$$ -->\n",
    "\n",
    "for some positive $\\beta_0$ that should ideally be greater than $\\lambda_1-\\lambda_0$. \n",
    "\n",
    "This new cost function can be interpreted as a constrained problem in which we minimize again $C_\\text{VQE}(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ but, this time, we add the constraint that the state has to be orthogonal to the previously obtained $|\\psi(\\vec{\\theta^0})\\rangle$ with $\\beta_0$ acting as a penalty term if orthogonality is not achieved. We are adding this constraint because the eigenstates of an observable (i.e. hermitian operator) corresponding to different eigenvalues are always mutually orthogonal â€”or can be made to be so in the event of degeneracy (i.e. repeated eigenvalues). It follows that, by enforcing orthogonality with the eigenstate corresponding to $\\lambda_0$, we are _effectively_ optimizing over the subspace that corresponds to the rest of the eigenvalues $\\{\\lambda_1, \\lambda_2,..., \\lambda_{N-1}\\}$; of which $\\lambda_1$ is the lowest and, therefore, from the variational theorem, the optimal solution of the new problem.\n",
    "\n",
    "This new problem can also be (more conveniently) interpreted as running VQE on the new observable:\n",
    "\n",
    "$$\n",
    "\\hat{H_1} := \\hat{H} + \\beta_0 |\\psi(\\vec{\\theta^0})\\rangle \\langle \\psi(\\vec{\\theta^0})|\n",
    "\\quad \\Rightarrow \\quad \n",
    "C_1(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H_1} | \\psi(\\vec{\\theta})\\rangle,\n",
    "$$\n",
    "\n",
    "<!-- \n",
    "Derivation:\n",
    "$$\n",
    "\\langle \\psi(\\vec{\\theta}) | \\hat{H_1} | \\psi(\\vec{\\theta})\\rangle = \n",
    "\\langle \\psi(\\vec{\\theta}) | \\bigg( \\hat{H} + \n",
    "\\beta_0 |\\psi(\\vec{\\theta^0})\\rangle \\langle \\psi(\\vec{\\theta^0})| \\bigg) | \\psi(\\vec{\\theta})\\rangle \\\\=\n",
    "\\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle + \n",
    "\\beta_0 \\langle\\psi(\\vec{\\theta}) |\\psi(\\vec{\\theta^0})\\rangle \\langle \\psi(\\vec{\\theta^0}) | \\psi(\\vec{\\theta})\\rangle \\\\=\n",
    "\\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle + \n",
    "\\beta_0 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^0})\\rangle  |^2 = \n",
    "C_1(\\vec{\\theta})\n",
    "$$\n",
    "-->\n",
    "\n",
    "Assuming that the solution to the new problem is $|\\psi(\\vec{\\theta^1})\\rangle$, the expected value of $\\hat{H}$ (not $\\hat{H_1}$) should be $ \\langle \\psi(\\vec{\\theta^1}) | \\hat{H} | \\psi(\\vec{\\theta^1})\\rangle = \\lambda_1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the third eigenvalue $\\lambda_2$, what you'd need to do is optimize the cost function:\n",
    "\n",
    "$$\n",
    "C_2(\\vec{\\theta}) := \n",
    "C_1(\\vec{\\theta}) + \\beta_1 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^1})\\rangle  |^2 \n",
    "% =\n",
    "% \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle + \n",
    "% \\beta_0 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^0})\\rangle  |^2 +  \n",
    "% \\beta_1 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^1})\\rangle  |^2\n",
    ",\n",
    "$$\n",
    "\n",
    "for some positive and big enough $\\beta_1$. This time we'd be enforcing orthogonality of the solution state to both $|\\psi(\\vec{\\theta^0})\\rangle$ and $|\\psi(\\vec{\\theta^1})\\rangle$ by penalizing those states in the search space which do not meet such requirement; _effectively_ restricting the search space. Consequently, the optimal solution of the new problem should be the eigenstate corresponding to $\\lambda_2$.\n",
    "\n",
    "Similarly to the previous case, this new problem can also be conveniently interpreted as VQE with the observable:\n",
    "\n",
    "$$\n",
    "\\hat{H_2} := \\hat{H_1} + \\beta_1 |\\psi(\\vec{\\theta^1})\\rangle \\langle \\psi(\\vec{\\theta^1})|\n",
    "\\quad \\Rightarrow \\quad \n",
    "C_2(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H_2} | \\psi(\\vec{\\theta})\\rangle.\n",
    "$$\n",
    "\n",
    "\n",
    "If the solution to this new problem is $|\\psi(\\vec{\\theta^2})\\rangle$, the expected value of $\\hat{H}$ (not $\\hat{H_2}$) should be $ \\langle \\psi(\\vec{\\theta^2}) | \\hat{H} | \\psi(\\vec{\\theta^2})\\rangle = \\lambda_2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Analogously, if you wanted to obtain the $k$-th eigenvalue $\\lambda_{k-1}$, you'd minimize\n",
    "\n",
    "$$\n",
    "C_{k-1}(\\vec{\\theta}) := \n",
    "C_{k-2}(\\vec{\\theta}) + \\beta_{k-2} |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^{k-2}})\\rangle  |^2,\n",
    "$$\n",
    "\n",
    "Remember that we defined $\\vec{\\theta^j}$ such that $\\langle \\psi(\\vec{\\theta^j}) | \\hat{H} | \\psi(\\vec{\\theta^j})\\rangle = \\lambda_j, \\forall j<k$. This problem would be equivalent to minimizing $C(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ but with the constraint that the state has to be orthogonal to $|\\psi(\\vec{\\theta^j})\\rangle \\; \\forall j \\in \\{0, \\cdots, k-1\\}$, therefore restricting the search space to the subspace corresponding to the eigenvalues $\\{\\lambda_{k-1},\\cdots,\\lambda_{N-1}\\}$.\n",
    "\n",
    "This problem is equivalent to a VQE with the observable\n",
    "\n",
    "$$\n",
    "\\hat{H}_{k-1} := \n",
    "\\hat{H}_{k-2} + \\beta_{k-2} |\\psi(\\vec{\\theta^{k-2}})\\rangle \\langle \\psi(\\vec{\\theta^{k-2}})|\n",
    "\\quad \\Rightarrow \\quad \n",
    "C_{k-1}(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H}_{k-1} | \\psi(\\vec{\\theta})\\rangle,\n",
    "$$\n",
    "\n",
    "As you can see from the process, to get the $k$-th eigenvalue, you need the (approximate) eigenstates of the previous $k-1$ eigenvalues, so you'd need to run VQE a total of $k$ times. This fact may be easier to visualize by looking at the expanded form of the previous formulas, originally expressed in recursive form:\n",
    "\n",
    "$$\n",
    "C_{k-1}(\\vec{\\theta}) = \n",
    "\\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle + \n",
    "\\sum_{j=0}^{k-2}\\beta_j |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^j})\\rangle  |^2, \\\\\n",
    "\\hat{H}_{k-1} := \n",
    "\\hat{H} + \\sum_{j=0}^{k-2}\\beta_j |\\psi(\\vec{\\theta^j})\\rangle \\langle \\psi(\\vec{\\theta^j})|. \n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Sampling Regression (QSR)\n",
    "\n",
    "One of the main issues with VQE is that, in order to get the parameters of the step $k$, you need the parameters from the step $k-1$ and so on, each one requiring a separate call to the quantum processor. This is a specially big problem when the access to the quantum devices is queued.\n",
    "\n",
    "A way to circumvent this inconvenience is to use more classical resources to be able to do the full optimization process in one call. This is where [Quantum Sampling Regression](https://arxiv.org/pdf/2012.02338) comes into play.\n",
    "\n",
    "The idea is that the cost function $C(\\theta) := \\langle \\psi(\\theta) | \\hat{H} | \\psi(\\theta)\\rangle$ can be written as a Fourier series in the following way:\n",
    "\n",
    "$$C(\\theta) = \\langle \\psi(\\theta) | \\hat{H} | \\psi(\\theta)\\rangle = a_0 + \\sum_{k=1}^S[a_k\\cos(k\\theta)+ b_k\\sin(k\\theta)] $$\n",
    "\n",
    "Depending on the periodicity and bandwidth of the original function, $S$ can be finite or infinite. We'll assume the latter to be the case.\n",
    "\n",
    "The next step is to sample the cost function $C(\\theta)$ several times to obtain the Fourier coefficients $\\{a_0, a_k, b_k\\}_{k=1}^S$. In particular, as we have $2S+1$ unknowns, we'll need to sample the cost function $2S+1$ times.\n",
    "\n",
    "Then, if we sample the cost function for $2S+1$ parameter values $\\{\\theta_1,...,\\theta_{2S+1}\\}$, you'd obtain the following system:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} 1 & \\cos(\\theta_1) & \\sin(\\theta_1) & \\cos(2\\theta_1) & ... & \\sin(S\\theta_1) \\\\\n",
    "1 & \\cos(\\theta_2) & \\sin(\\theta_2) & \\cos(2\\theta_2) & \\cdots & \\sin(S\\theta_2)\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "1 & \\cos(\\theta_{2S+1}) & \\sin(\\theta_{2S+1}) & \\cos(2\\theta_{2S+1}) & \\cdots & \\sin(S\\theta_{2S+1})\n",
    "\\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\\\ b_1 \\\\ a_2 \\\\ \\vdots \\\\ b_S \\end{pmatrix} = \\begin{pmatrix} C(\\theta_1) \\\\ C(\\theta_2) \\\\ \\vdots \\\\ C(\\theta_{2S+1}) \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "that we'll rewrite as \n",
    "\n",
    "$$\n",
    "Fa=c.\n",
    "$$\n",
    "\n",
    "In practice, this system is generally not consistent, as the cost function values $c$ are not exact, so it is usually a good idea to normalize them by multiplying by $F^\\dagger$ on the left, obtaining:\n",
    "\n",
    "$$\n",
    "F^\\dagger Fa = F^\\dagger c.\n",
    "$$\n",
    "\n",
    "This new system is always consistent and its solution is a least-squares solution to the original problem.\n",
    "\n",
    "If instead of only one parameter we have $k$, each parameter $\\theta^i$ will have its own $S_i$ for $i\\in \\{1,...,k\\}$, so the total number of samples needed would be:\n",
    "\n",
    "$$\n",
    "T=\\prod_{i=1}^k(2S_i+1)\\leq \\prod_{i=1}^k(2S_{max}+1) = (2S_{max}+1)^n,\n",
    "$$\n",
    "\n",
    "where $S_{max}=\\max_i(S_i)$.\n",
    "\n",
    "Furthermore, playing around with $S_{max}$ as a tunable parameter (i.e. instead to inferring it) opens up new possibilities like:\n",
    "\n",
    "- _Oversampling_: to improve accuracy.\n",
    "- _Undersampling_: to boost performance by reducing runtime overhead, or eliminating local minima.\n",
    "\n",
    "Refinement and combinations with other techniques are also possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
