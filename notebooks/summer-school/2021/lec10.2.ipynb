{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Capacity and Power of Quantum Machine Learning Models & the Future of Quantum Machine Learning\n",
    "\n",
    "<div class=\"youtube-wrapper\">\n",
    "    <iframe src=\"https://www.youtube.com/embed/1-IrbRR4rwM\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\n",
    "\n",
    "In this lecture, Amira presents different open questions regarding quantum machine learning and quantum computing in general. The aim is to explain the capacity and power of quantum machine learning for applications today and tomorrow. She begins by focusing on the definition of capacity in the field of classical machine learning. Indeed, it has a lot of definition: statistical complexity, expressivity, power. That can be summarized by wondering how many functions the model can approximate. The more it can approximate, the more capacity it has. However, counterintuitively, higer capacity is not necessarly a good thing. Capacity is linked to generalization related to the bias/variance tradeoff where we want the model neither to underfit nor to overfit. Thus the optimal capacity is a model with the lowest generalization error in the bias/variance tradeoff. Thus the question of how to measure capacity of a machine learning model in classical as well as quantum computing is very important and open.\n",
    "\n",
    "### Suggested links\n",
    "\n",
    " - Download the lecturer's notes [here](/content/summer-school/2021/resources/lecture-notes/Lecture10.2.pdf)\n",
    "\n",
    " - Read Analytics India Magazine on [Big Data To Good Data](https://analyticsindiamag.com/big-data-to-good-data-andrew-ng-urges-ml-community-to-be-more-data-centric-and-less-model-centric/)\n",
    " - Watch IBM - Qiskit on [The Future of Quantum Machine Learning](https://www.youtube.com/watch?v=5UsJV2BNj2U&ab_channel=Qiskit)\n",
    "\n",
    "### Other resources\n",
    "\n",
    " - Read Zhang et al. on [Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)\n",
    " - Read Bartlett et al. on [Spectrally-normalized margin bounds for neural networks](https://arxiv.org/abs/1706.08498)\n",
    " - Read Neyshabur et al. on [A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks](https://arxiv.org/abs/1707.09564)\n",
    " - Read Keskar et al. on [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836)\n",
    " - Read Sim et al. on [Expressibility and entangling capability of parameterized quantum circuits for hybrid quantum-classical algorithms](https://arxiv.org/abs/1905.10876)\n",
    " - Read McClean et al. on [Barren plateaus in quantum neural network training landscapes](https://arxiv.org/abs/1803.11173)\n",
    " - Read Schuld et al. on [The effect of data encoding on the expressive power of variational quantum machine learning models](https://arxiv.org/abs/2008.08605)\n",
    " - Read Pérez-Salinas et al. on [Data re-uploading for a universal quantum classifier](https://arxiv.org/abs/1907.02085)\n",
    " - Read Abbas et al. on [The power of quantum neural networks](https://arxiv.org/abs/2011.00027)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
