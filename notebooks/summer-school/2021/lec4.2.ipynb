{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Classical Machine Learning:\n",
    "\n",
    "<div class=\"youtube-wrapper\">\n",
    "    <iframe src=\"https://www.youtube.com/embed/lpPij21jnZ4\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n",
    "</div>\n",
    "\n",
    "\n",
    "This is the second session of the classical machine learning (ML) introduction presented by Amira. She presents a brief of history of ML, then introduces simple ML methods, i.e., linear regression based models. Amira introduces neural networks, starting with simple perceptron and then illustrating Feed Forward Neural Networks (FFNNs). In the final section, Amira introduces Supper Vector Machines (SVM) concepts for classification type tasks. Amira introduces a very high level overview of Quantum Machine Learning, with the quadrants showing both classical and quantum machine learning in terms of Data Processing device as one axis and the Data generating system as the remaining axis.\n",
    "\n",
    "### Suggested links\n",
    "\n",
    " - Download the lecturer's notes [here](/content/summer-school/2021/resources/lecture-notes/QGSS2021_Lecture4.2_LectureNotes.pdf)\n",
    "\n",
    "<!-- ::: q-block.reminder -->\n",
    "\n",
    "### FAQ\n",
    "\n",
    "<details>\n",
    "  <summary>How are linear models related to the standard statistical methods of least squares for regression analysis?</summary>\n",
    "They are the same\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Are the activation functions in each layer the same or different?</summary>\n",
    "- \"It can be differnt\" - Boniface Yogendran\n",
    "- \"Usually last activation function is different rest all are kept same.\" - Edwin\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Just confirming...The more data there is the easier the higher accuracy the model has? How to combat lack of available data especialy when create a CNN network?</summary>\n",
    "One way to combat is replicating present data and augment that data. Maybe vertical flip, horizontal flip, crop, blur, etc.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Do linear models in general get more/less accurate when taking more features into account?</summary>\n",
    "If you'll take more more features, it'll result in overfitting. If less, then you'll have biased output.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>On the regression model, does theta include the magnitude of the intercept?</summary>\n",
    "Yes, it's $\\theta_0$\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>During the lecture it was stated that one can easily add or out a bias, how does this work? Could you please explain?</summary>\n",
    "- A1: For example, think about on a linear model, you can change the value of the intercept to move the boundary within the plane.\n",
    "\n",
    "- A2: The main function of a bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node receives). You can achieve that with a single bias node with connections to N nodes, or with N bias nodes each with a single connection; the result should be the same\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Is it correct to think of neural networks (NN) as models that can be used for non-linear fitting, in general? Would it an overkill to use NNs for datasets with linear dependencies?</summary>\n",
    "NN requires lots of data and computationally expensive. If its a linear model, we can get away with less computationally expensive ML methods.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Is there a rule of thumb in the selection of the weights and thus the number of neurons?</summary>\n",
    "Actually in practice, number of neurons in each layer is 2^x. Number of layers is your call. Weights are automatically selected for you during backpropagation (optimization). You just initialize weight vector with random numbers. Thats it.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Does noise affect all these models?</summary>\n",
    "Yes it does. It won't if your model is generalized aptly (which hasn't been achieved yet as still in one way or the other, models are affected by some kind of a noise)\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Can we use a circular or elliptical function instead of a feature map for data that is not separable linearly?</summary>\n",
    "Yes. feature mapping is easier.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Can we use a circular or elliptical function instead of a feature map for data that is not separable linearly?</summary>\n",
    "Yes. feature mapping is easier.\n",
    "</details>\n",
    "\n",
    "### Live Q&A\n",
    "\n",
    "<details>\n",
    "  <summary>What are the different ways to map the data in higher dimensions?</summary>\n",
    "Answer was provided at timestamp 2m 29s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Could you please explain again what a kernel is?</summary>\n",
    "Answer was provided at timestamp 4m 48s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>How do we decide the Bias ? and does it remain same throughout training?</summary>\n",
    "Answer was provided at timestamp 9m 30s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Someone in the previous video wanted a bit more clarification on why the dual formulation is useful</summary>\n",
    "Answer was provided at timestamp 10m 40s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>What if, after applying the feature map, data is still not linearly separable?</summary>\n",
    "Answer was provided at timestamp 13m 4s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Why do we not have a different activation function for each parameter set ?</summary>\n",
    "Answer was provided at timestamp 14m 46s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>How can we choose a good activation function for a FFNN ?</summary>\n",
    "Answer was provided at timestamp 17m 21s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>How do we incorporate the optimization of the distance between the linear model and nearest points in each class into the SVM ?</summary>\n",
    "Answer was provided at timestamp 20m 5s in the Lecture 4.2 Live Q&amp;A session\n",
    "</details>\n",
    "\n",
    "<!-- ::: -->\n",
    "\n",
    "### Suggested links\n",
    "\n",
    "- Read James Lighthill (1973) on [Artificial Intelligence: A General Survey (Lighthill Report)](https://en.wikipedia.org/wiki/Lighthill_report)\n",
    "- Read Seppo Linnainmaa Thesis (1970) on [Algoithm Kumulatiivinen Pyöristysvirhe](https://people.idsia.ch//~juergen/linnainmaa1970thesis.pdf)\n",
    "- Watch MIT Opencourseware on [Learning: Support Vector Machines](https://www.youtube.com/watch?v=_PwhiWxHK8o&t=1123s)\n",
    "- Read Stanford University Statistics 315a Modern Applied Statistics course material on [Glossary for terminology used in Machine Learning versus Statistics](http://statweb.stanford.edu/~tibs/stat315a/glossary.pdf)\n",
    "- Read Saptashwa Bhattacharyya on [Support Vector Machine: Complete Theory](https://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e)\n",
    "- Read Saptashwa Bhattacharyya on [Support Vector Machine: Kernel Trick; Mercer’s Theorem](https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d)\n",
    "- Read Support Vector Machines (SVM) Tutorial on [Note: above link was posted as useful link for visual representation](https://web.mit.edu/zoya/www/SVM.pdf)\n",
    "- Watch Michael J. Biercuk on [Improving and automating quantum computers with machine learning](https://youtu.be/G_UMvI2bASg)\n",
    "- Read Qiskit on [Feature Maps](https://qiskit.org/documentation/apidoc/qiskit.aqua.components.feature_maps.html)\n",
    "- Read Google on [Machine Learning Crash Course with TensorFlow APIs](https://developers.google.com/machine-learning/crash-course)\n",
    "- Read [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition](https://learning.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "- Read [Deep Learning Specialization](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome)\n",
    "- Read [Google ML Crash Course](https://developers.google.com/machine-learning/crash-course/ml-intro)\n",
    "- Watch [MIT Introduction to Deep Learning](https://www.youtube.com/playlist?list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi)\n",
    "- Read [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
    "- Read [Stanford ML Course](https://www.coursera.org/learn/machine-learning)\n",
    "- Watch [3Blue1Brown series on Neural Nets (Gradient descent, how neural networks learn | Chapter 2, Deep learning)](https://www.youtube.com/watch?v=IHZwWFHWa-w)\n",
    "- Read [Tinker With a Neural Network Right Here in Your Browser.](https://playground.tensorflow.org)\n",
    "- Read [Comparison of activation functions](https://en.wikipedia.org/wiki/Activation_function#Comparison_of_activation_functions)\n",
    "- Read [Detailed responses in Stackoverflow on “What is the role of the bias in neural networks?“](https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks)\n",
    "- Read [Pattern Recognition and Machine Learning (by Christopher Bishop)](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)\n",
    "- Watch [StatQuest with Josh Starmer](https://www.youtube.com/c/joshstarmer)\n",
    "- Read [Deep Learning related introductory content by Jon Krohn](https://www.jonkrohn.com/)\n",
    "\n",
    "\n",
    "### Suggested Reading\n",
    "- _Deep Learning_ by Ian Goodfello et al.\n",
    "- _Neural Network Youtube Series_ by 3blue1brown\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
